{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"GAN_oversample.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN6uI4zWf1AxCbv8fBZjHTK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"PomF9YN0e0gR","executionInfo":{"status":"ok","timestamp":1618791609506,"user_tz":-120,"elapsed":3591,"user":{"displayName":"Shirwan Piroti","photoUrl":"","userId":"12026415780371804948"}}},"source":["#Important Libraries\n","from zipfile import ZipFile\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","\n","#Data Encoding\n","from sklearn.preprocessing import  OneHotEncoder\n","\n","#Preprocessing\n","from sklearn.preprocessing import StandardScaler\n","\n","#Classifier\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"CFGZ1QzkfuxK"},"source":["#Synthetic dataset\n","synth_train = pd.read_csv('/content/drive/MyDrive/Degree Project/train.zip', delimiter=',')\n","synth_test = pd.read_csv('/content/drive/MyDrive/Degree Project/test.zip', delimiter=',')\n","def data(df):\n","  Y = df['Label']                                  #Labels\n","  Y = Y.to_numpy()\n","  X = df.to_numpy()\n","  X = np.delete(X, -1, axis=1)\n","  X1 = X[:,0:2]                                    #Categorical Features\n","  X2 = X[:, 2:]                                    #Numerical Features\n","  X1 = OneHotEncoder().fit_transform(X1).toarray() #Encode categorical features\n","  X = np.concatenate((X1,X2), axis=1)\n","  X = StandardScaler().fit_transform(X)            #Standardizes data\n","  X = np.asarray(X).astype('float32')\n","  Y = np.asarray(Y).astype('float32')\n","  return X,Y\n","\n","X_train, Y_train = data(synth_train)\n","X_test, Y_test = data(synth_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Domp7R6yfxLf"},"source":["#KDD-Cup 99 dataset\n","KDD_train = pd.read_csv('/content/drive/MyDrive/Degree Project/kddcup.data_10_percent.gz', compression='gzip').dropna() \n","KDD_test = pd.read_csv('/content/drive/MyDrive/Degree Project/corrected.gz', compression='gzip').dropna()\n","KDD_train.columns = [\n","    'duration',\n","    'protocol_type',\n","    'service',\n","    'flag',\n","    'src_bytes',\n","    'dst_bytes',\n","    'land',\n","    'wrong_fragment',\n","    'urgent',\n","    'hot',\n","    'num_failed_logins',\n","    'logged_in',\n","    'num_compromised',\n","    'root_shell',\n","    'su_attempted',\n","    'num_root',\n","    'num_file_creations',\n","    'num_shells',\n","    'num_access_files',\n","    'num_outbound_cmds',\n","    'is_host_login',\n","    'is_guest_login',\n","    'count',\n","    'srv_count',\n","    'serror_rate',\n","    'srv_serror_rate',\n","    'rerror_rate',\n","    'srv_rerror_rate',\n","    'same_srv_rate',\n","    'diff_srv_rate',\n","    'srv_diff_host_rate',\n","    'dst_host_count',\n","    'dst_host_srv_count',\n","    'dst_host_same_srv_rate',\n","    'dst_host_diff_srv_rate',\n","    'dst_host_same_src_port_rate',\n","    'dst_host_srv_diff_host_rate',\n","    'dst_host_serror_rate',\n","    'dst_host_srv_serror_rate',\n","    'dst_host_rerror_rate',\n","    'dst_host_srv_rerror_rate',\n","    'outcome'\n","]\n","\n","KDD_test.columns = KDD_train.columns\n","#Remove duplicates\n","KDD_train = KDD_train.drop_duplicates(keep=False)\n","KDD_test = KDD_test.drop_duplicates(keep=False)\n","#Change outcome to 0 if normal and 1 if anomalous\n","KDD_train['outcome'] = (KDD_train['outcome']!='normal.')*1\n","KDD_test['outcome'] = (KDD_test['outcome']!='normal.')*1\n","def data(df):\n","  X = df.to_numpy()\n","  Y = X[:,-1]\n","  X = np.delete(X, -1, axis=1)\n","  X1 = np.array([X[:,0]]).transpose()\n","  X2 = OneHotEncoder().fit_transform(X[:,1:2]).toarray()\n","  X3 = X[:,4:]\n","  X = np.concatenate((X1,X2,X3), axis=1)\n","  X = np.asarray(X).astype('float32')\n","  Y = np.asarray(Y).astype('float32')\n","  X = StandardScaler().fit_transform(X)\n","  normal_indx = np.where(Y==0)                      #Index of normal observations\n","  anomaly_indx = np.where(Y==1)                     #Index of anomalies\n","  X_normal  = X[normal_indx]\n","  X_anomaly = X[anomaly_indx]\n","  Y_normal  = Y[normal_indx]\n","  Y_anomaly = Y[anomaly_indx]\n","  return X,X_normal,X_anomaly,Y,Y_normal,Y_anomaly\n","\n","X_train,X_normal_train,X_anomaly_train,Y_train,Y_normal_train,Y_anomaly_train = data(KDD_train)\n","X_test,X_normal_test,X_anomaly_test,Y_test,Y_normal_test,Y_anomaly_test       = data(KDD_test) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iGfkghjsfzcD"},"source":["class GAN(keras.Model):\n","  def __init__(self,latent_dim=20, init_kernel=keras.initializers.GlorotNormal, init_bias=keras.initializers.constant(0)):\n","    self.latent_dim    = latent_dim\n","    self.generator     = self.Generator()\n","    self.discriminator = self.Discriminator\n","\n","    def Generator(self):\n","      generator_input  = keras.Input(self.latent_dim, name='z') #z\n","      generator        = layers.Dense(128, activation='relu', kernel_initializer=self.init_kernel, bias_initializer=self.init_bias)(generator_input)\n","      generator        = layers.Dense(256, activation='relu', kernel_initializer=self.init_kernel, bias_initializer=self.init_bias)(generator)\n","      generator        = layers.Dense(128, activation='relu', kernel_initializer=self.init_kernel, bias_initializer=self.init_bias)(generator)\n","      generator        = layers.Dense(64, activation='relu', kernel_initializer=self.init_kernel, bias_initializer=self.init_bias)(generator)\n","      generator_output = layers.Dense(self.data_dim, activation='linear', kernel_initializer=self.init_kernel, bias_initializer=self.init_bias)(generator) #G(z)\n","      return keras.Model(generator_input, generator_output, name='Generator')\n","\n","    def Discriminator(self):\n","      D_input  = keras.Input(self.generator.output.shape[1])\n","      D  = layers.Dense(128, activation='relu', kernel_initializer=self.init_kernel, bias_initializer=self.init_bias)(D_input)\n","      D  = layers.Dense(256, activation='relu', kernel_initializer=self.init_kernel, bias_initializer=self.init_bias)(D)\n","      D  = layers.Dense(128, activation='relu', kernel_initializer=self.init_kernel, bias_initializer=self.init_bias)(D)\n","      D_output = layers.Dense(1, activation='sigmoid', kernel_initializer=self.init_kernel, bias_initializer=self.init_bias)(D)\n","      return keras.Model(x, D_output, name='Discriminator')\n","\n","    def compile(self,optimizer = keras.optimizers.Adam(learning_rate=10**-5, beta_1=0.5), loss = keras.losses.BinaryCrossentropy(), train_metric = keras.metrics.BinaryCrossentropy):\n","      self.optimizer    = optimizer\n","      self.loss         = loss\n","\n","    def train(self,X_train,Epochs=50,batch_size=128):\n","        for Epoch in range(Epochs):\n","        X = tf.data.Dataset.from_tensor_slices(X_train)\n","        X = X.shuffle(buffer_size=1024).batch(batch_size) #Shuffles data and divides the dataset in batches\n","        pbar = tqdm(X, position=0, leave=True)            #Progressbar\n","        for step, x in enumerate(pbar):\n","          z = tf.random.normal(shape=(x.shape[0], self.latent_dim))     #Sample normal distributed noise\n","          with tf.GradientTape(persistent=True) as tape:\n","            x_ = self.generator(z)                                      #Generate x from noise              \n","            real_pred = self.discriminator(x_)\n","            fake_pred = self.discriminator(z)\n","            d_loss = self.loss(tf.ones_like(real_pred), real_pred)+self.loss(tf.zeros_like(fake_pred), fake_pred) #Discriminator loss\n","            g_loss = self.loss(tf.ones_like(fake_pred), fake_pred) #Generator loss\n","          \n","          d_gradients = tape.gradient(d_loss, self.discriminator.trainable_weights)               #Discriminator gradients\n","          self.optimizer.apply_gradients(zip(d_gradients, self.discriminator.trainable_weights))  #Update Discriminator paramaters\n","          g_gradients = tape.gradient(g_loss, self.generator.trainable_weights)                   #Generator & Encoder loss\n","          self.optimizer.apply_gradients(zip(g_gradients, self.generator.trainable_weights))      #Update Generator and Encoder parameters\n","          del tape\n","    \n","    def sample(self,X_train):"],"execution_count":null,"outputs":[]}]}